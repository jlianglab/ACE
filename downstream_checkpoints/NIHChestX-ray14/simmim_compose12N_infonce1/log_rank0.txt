[2024-03-21 17:32:42 swin_base_patch4_window7_224] (main_classification_ddp.py 596): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:32:42 swin_base_patch4_window7_224] (main_classification_ddp.py 599): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /sda1/zhouziyu/ssl/NIHChestX-ray14_pretrain/checkpoints/compose/checkpoint0050_comp_decomp_12Ncode.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.25e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2024-03-21 17:32:42 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/sda1/zhouziyu/ssl/NIHChestX-ray14_pretrain/checkpoints/compose/checkpoint0050_comp_decomp_12Ncode.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:32:48 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:32:49 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:32:49 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:32:49 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:32:50 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /sda1/zhouziyu/ssl/NIHChestX-ray14_pretrain/checkpoints/compose/checkpoint0050_comp_decomp_12Ncode.pth for fine-tuning......
[2024-03-21 17:34:16 swin_base_patch4_window7_224] (main_classification_ddp.py 596): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:34:16 swin_base_patch4_window7_224] (main_classification_ddp.py 599): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.25e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2024-03-21 17:34:16 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:34:21 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:34:23 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:34:23 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:34:23 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:34:24 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:34:40 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:34:40 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:35:47 swin_base_patch4_window7_224] (main_classification_ddp.py 596): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:35:47 swin_base_patch4_window7_224] (main_classification_ddp.py 599): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:35:47 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:35:52 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:35:53 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:35:53 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:35:53 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:35:54 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:35:55 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:35:55 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:35:55 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:38:44 swin_base_patch4_window7_224] (main_classification_ddp.py 597): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:38:44 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:38:44 swin_base_patch4_window7_224] (main_classification_ddp.py 601): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:38:49 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:38:50 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:38:50 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:38:50 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:38:51 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:38:52 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:38:52 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:38:52 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:40:02 swin_base_patch4_window7_224] (main_classification_ddp.py 597): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:40:02 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:40:02 swin_base_patch4_window7_224] (main_classification_ddp.py 601): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:40:08 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:40:09 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:40:09 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:40:09 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:40:10 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:40:11 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:40:11 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:40:11 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:44:39 swin_base_patch4_window7_224] (main_classification_ddp.py 597): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:44:40 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:44:40 swin_base_patch4_window7_224] (main_classification_ddp.py 601): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:44:57 swin_base_patch4_window7_224] (main_classification_ddp.py 597): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:44:57 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:44:57 swin_base_patch4_window7_224] (main_classification_ddp.py 601): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:45:02 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:45:04 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:45:04 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:45:04 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:45:05 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:45:06 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:45:06 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:45:06 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:46:10 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:46:10 swin_base_patch4_window7_224] (main_classification_ddp.py 603): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:46:10 swin_base_patch4_window7_224] (main_classification_ddp.py 604): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:46:15 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:46:17 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:46:17 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:46:17 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:46:18 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:46:19 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:46:19 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:46:19 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:46:58 swin_base_patch4_window7_224] (main_classification_ddp.py 600): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:46:58 swin_base_patch4_window7_224] (main_classification_ddp.py 603): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:46:58 swin_base_patch4_window7_224] (main_classification_ddp.py 604): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:47:03 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:47:04 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:47:04 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:47:04 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:47:05 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:47:06 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:47:06 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:47:06 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:48:00 swin_base_patch4_window7_224] (main_classification_ddp.py 601): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:48:00 swin_base_patch4_window7_224] (main_classification_ddp.py 604): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:48:00 swin_base_patch4_window7_224] (main_classification_ddp.py 605): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:48:06 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:48:07 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:48:07 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:48:07 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:48:08 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:48:09 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:48:09 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:48:10 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:48:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][0/2433]	eta 2:13:45 lr 0.000000	 wd 0.0500	time 3.2985 (3.2985)	loss 0.8011 (0.8011)	grad_norm 5.8001 (5.8001)	loss_scale 65536.0000 (65536.0000)	mem 5410MB
[2024-03-21 17:48:21 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][10/2433]	eta 0:41:38 lr 0.000000	 wd 0.0500	time 0.8083 (1.0313)	loss 0.7826 (0.7872)	grad_norm 5.8390 (5.8988)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:48:34 swin_base_patch4_window7_224] (main_classification_ddp.py 601): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 17:48:34 swin_base_patch4_window7_224] (main_classification_ddp.py 604): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 17:48:34 swin_base_patch4_window7_224] (main_classification_ddp.py 605): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 17:48:39 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 17:48:41 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 17:48:41 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 17:48:41 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 17:48:42 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 17:48:43 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 17:48:43 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 17:48:43 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 17:48:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][0/2433]	eta 2:13:13 lr 0.000000	 wd 0.0500	time 3.2855 (3.2855)	loss 0.7423 (0.7423)	grad_norm 5.5610 (5.5610)	loss_scale 65536.0000 (65536.0000)	mem 5410MB
[2024-03-21 17:48:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][10/2433]	eta 0:41:35 lr 0.000000	 wd 0.0500	time 0.8034 (1.0301)	loss 0.7460 (0.7347)	grad_norm 5.2816 (5.6530)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:02 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][20/2433]	eta 0:37:08 lr 0.000000	 wd 0.0500	time 0.8095 (0.9236)	loss 0.7383 (0.7346)	grad_norm 5.8642 (5.6103)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][30/2433]	eta 0:35:33 lr 0.000000	 wd 0.0500	time 0.8176 (0.8877)	loss 0.7026 (0.7332)	grad_norm 5.7952 (5.6450)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][40/2433]	eta 0:34:42 lr 0.000000	 wd 0.0500	time 0.8189 (0.8701)	loss 0.6747 (0.7278)	grad_norm 5.1474 (5.6225)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:27 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][50/2433]	eta 0:34:09 lr 0.000000	 wd 0.0500	time 0.8160 (0.8600)	loss 0.7081 (0.7245)	grad_norm 4.8938 (5.5962)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][60/2433]	eta 0:33:43 lr 0.000000	 wd 0.0500	time 0.8185 (0.8529)	loss 0.6950 (0.7217)	grad_norm 5.6240 (5.5905)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][70/2433]	eta 0:33:24 lr 0.000000	 wd 0.0500	time 0.8180 (0.8481)	loss 0.6926 (0.7176)	grad_norm 5.2029 (5.5708)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:49:51 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][80/2433]	eta 0:33:07 lr 0.000000	 wd 0.0500	time 0.8189 (0.8447)	loss 0.6819 (0.7131)	grad_norm 5.1903 (5.5366)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][90/2433]	eta 0:32:53 lr 0.000000	 wd 0.0500	time 0.8252 (0.8425)	loss 0.6807 (0.7091)	grad_norm 5.3852 (5.5188)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][100/2433]	eta 0:32:41 lr 0.000000	 wd 0.0500	time 0.8244 (0.8407)	loss 0.6609 (0.7048)	grad_norm 5.4192 (5.5003)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][110/2433]	eta 0:32:29 lr 0.000000	 wd 0.0500	time 0.8295 (0.8393)	loss 0.6343 (0.6994)	grad_norm 5.1119 (5.4705)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][120/2433]	eta 0:32:18 lr 0.000000	 wd 0.0500	time 0.8293 (0.8382)	loss 0.6172 (0.6939)	grad_norm 4.8325 (5.4369)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:33 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][130/2433]	eta 0:32:08 lr 0.000000	 wd 0.0500	time 0.8300 (0.8373)	loss 0.5988 (0.6882)	grad_norm 5.0131 (5.4023)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][140/2433]	eta 0:31:58 lr 0.000000	 wd 0.0500	time 0.8261 (0.8366)	loss 0.6162 (0.6831)	grad_norm 5.1753 (5.3755)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][150/2433]	eta 0:31:48 lr 0.000000	 wd 0.0500	time 0.8266 (0.8360)	loss 0.5968 (0.6776)	grad_norm 4.8434 (5.3418)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:50:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][160/2433]	eta 0:31:39 lr 0.000000	 wd 0.0500	time 0.8325 (0.8357)	loss 0.5517 (0.6713)	grad_norm 4.7120 (5.3080)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][170/2433]	eta 0:31:30 lr 0.000000	 wd 0.0500	time 0.8323 (0.8354)	loss 0.5531 (0.6645)	grad_norm 4.4741 (5.2620)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][180/2433]	eta 0:31:21 lr 0.000000	 wd 0.0500	time 0.8279 (0.8351)	loss 0.5398 (0.6582)	grad_norm 4.5269 (5.2255)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][190/2433]	eta 0:31:12 lr 0.000000	 wd 0.0500	time 0.8310 (0.8349)	loss 0.5762 (0.6519)	grad_norm 4.4204 (5.1866)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][200/2433]	eta 0:31:03 lr 0.000000	 wd 0.0500	time 0.8305 (0.8347)	loss 0.5104 (0.6455)	grad_norm 4.0695 (5.1428)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:39 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][210/2433]	eta 0:30:55 lr 0.000000	 wd 0.0500	time 0.8389 (0.8347)	loss 0.5154 (0.6390)	grad_norm 4.0900 (5.0952)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][220/2433]	eta 0:30:46 lr 0.000000	 wd 0.0500	time 0.8316 (0.8346)	loss 0.5122 (0.6323)	grad_norm 4.3636 (5.0548)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:51:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][230/2433]	eta 0:30:38 lr 0.000000	 wd 0.0500	time 0.8314 (0.8345)	loss 0.4745 (0.6255)	grad_norm 4.2372 (5.0109)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][240/2433]	eta 0:30:30 lr 0.000000	 wd 0.0500	time 0.8309 (0.8345)	loss 0.4583 (0.6187)	grad_norm 3.9073 (4.9651)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][250/2433]	eta 0:30:21 lr 0.000000	 wd 0.0500	time 0.8336 (0.8345)	loss 0.4229 (0.6122)	grad_norm 3.5162 (4.9186)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:21 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][260/2433]	eta 0:30:13 lr 0.000000	 wd 0.0500	time 0.8348 (0.8345)	loss 0.4460 (0.6051)	grad_norm 3.4637 (4.8731)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][270/2433]	eta 0:30:05 lr 0.000000	 wd 0.0500	time 0.8368 (0.8346)	loss 0.3944 (0.5980)	grad_norm 3.8211 (4.8291)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][280/2433]	eta 0:29:56 lr 0.000000	 wd 0.0500	time 0.8370 (0.8346)	loss 0.3704 (0.5909)	grad_norm 3.2607 (4.7800)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][290/2433]	eta 0:29:48 lr 0.000000	 wd 0.0500	time 0.8326 (0.8346)	loss 0.4130 (0.5841)	grad_norm 3.2721 (4.7330)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:52:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][300/2433]	eta 0:29:40 lr 0.000000	 wd 0.0500	time 0.8374 (0.8347)	loss 0.3960 (0.5779)	grad_norm 3.1646 (4.6842)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:03 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][310/2433]	eta 0:29:32 lr 0.000000	 wd 0.0500	time 0.8331 (0.8347)	loss 0.3710 (0.5711)	grad_norm 3.3079 (4.6368)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][320/2433]	eta 0:29:23 lr 0.000000	 wd 0.0500	time 0.8348 (0.8348)	loss 0.3738 (0.5647)	grad_norm 3.0411 (4.5891)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][330/2433]	eta 0:29:15 lr 0.000000	 wd 0.0500	time 0.8389 (0.8349)	loss 0.3240 (0.5578)	grad_norm 3.2252 (4.5397)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][340/2433]	eta 0:29:07 lr 0.000000	 wd 0.0500	time 0.8337 (0.8349)	loss 0.3569 (0.5514)	grad_norm 2.6538 (4.4889)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][350/2433]	eta 0:28:59 lr 0.000000	 wd 0.0500	time 0.8344 (0.8350)	loss 0.3263 (0.5453)	grad_norm 2.6876 (4.4390)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:45 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][360/2433]	eta 0:28:50 lr 0.000000	 wd 0.0500	time 0.8370 (0.8350)	loss 0.3266 (0.5389)	grad_norm 2.5307 (4.3902)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:53:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][370/2433]	eta 0:28:42 lr 0.000000	 wd 0.0500	time 0.8362 (0.8351)	loss 0.3120 (0.5326)	grad_norm 2.5490 (4.3433)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][380/2433]	eta 0:28:34 lr 0.000000	 wd 0.0500	time 0.8396 (0.8351)	loss 0.2452 (0.5264)	grad_norm 2.4301 (4.2943)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][390/2433]	eta 0:28:26 lr 0.000000	 wd 0.0500	time 0.8391 (0.8352)	loss 0.2780 (0.5203)	grad_norm 2.0777 (4.2449)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][400/2433]	eta 0:28:18 lr 0.000000	 wd 0.0500	time 0.8409 (0.8353)	loss 0.2868 (0.5146)	grad_norm 2.1843 (4.1949)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][410/2433]	eta 0:28:09 lr 0.000000	 wd 0.0500	time 0.8396 (0.8354)	loss 0.2828 (0.5089)	grad_norm 2.3423 (4.1474)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][420/2433]	eta 0:28:01 lr 0.000000	 wd 0.0500	time 0.8348 (0.8355)	loss 0.2388 (0.5032)	grad_norm 2.4644 (4.1001)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][430/2433]	eta 0:27:53 lr 0.000000	 wd 0.0500	time 0.8375 (0.8355)	loss 0.2623 (0.4977)	grad_norm 1.9396 (4.0489)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:54:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][440/2433]	eta 0:27:45 lr 0.000000	 wd 0.0500	time 0.8365 (0.8356)	loss 0.2490 (0.4922)	grad_norm 2.0017 (4.0010)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][450/2433]	eta 0:27:37 lr 0.000000	 wd 0.0500	time 0.8424 (0.8356)	loss 0.2663 (0.4867)	grad_norm 1.7400 (3.9537)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][460/2433]	eta 0:27:28 lr 0.000000	 wd 0.0500	time 0.8427 (0.8357)	loss 0.2039 (0.4815)	grad_norm 1.6880 (3.9071)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:17 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][470/2433]	eta 0:27:20 lr 0.000000	 wd 0.0500	time 0.8424 (0.8358)	loss 0.2105 (0.4763)	grad_norm 1.7218 (3.8608)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:25 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][480/2433]	eta 0:27:12 lr 0.000000	 wd 0.0500	time 0.8370 (0.8358)	loss 0.1885 (0.4708)	grad_norm 1.4727 (3.8137)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:33 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][490/2433]	eta 0:27:04 lr 0.000000	 wd 0.0500	time 0.8379 (0.8359)	loss 0.2239 (0.4657)	grad_norm 1.6408 (3.7675)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][500/2433]	eta 0:26:55 lr 0.000000	 wd 0.0500	time 0.8389 (0.8360)	loss 0.1910 (0.4609)	grad_norm 1.4736 (3.7225)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:50 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][510/2433]	eta 0:26:47 lr 0.000000	 wd 0.0500	time 0.8402 (0.8360)	loss 0.1959 (0.4562)	grad_norm 1.6314 (3.6783)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:55:59 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][520/2433]	eta 0:26:39 lr 0.000000	 wd 0.0500	time 0.8378 (0.8361)	loss 0.2116 (0.4513)	grad_norm 1.3907 (3.6366)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:07 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][530/2433]	eta 0:26:31 lr 0.000000	 wd 0.0500	time 0.8342 (0.8362)	loss 0.1966 (0.4467)	grad_norm 1.3623 (3.5935)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:15 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][540/2433]	eta 0:26:23 lr 0.000000	 wd 0.0500	time 0.8360 (0.8362)	loss 0.2183 (0.4421)	grad_norm 1.3253 (3.5518)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][550/2433]	eta 0:26:14 lr 0.000000	 wd 0.0500	time 0.8381 (0.8363)	loss 0.2454 (0.4379)	grad_norm 1.3112 (3.5106)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][560/2433]	eta 0:26:06 lr 0.000000	 wd 0.0500	time 0.8454 (0.8364)	loss 0.1789 (0.4336)	grad_norm 1.2412 (3.4705)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][570/2433]	eta 0:25:58 lr 0.000000	 wd 0.0500	time 0.8357 (0.8364)	loss 0.2083 (0.4295)	grad_norm 1.4028 (3.4314)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][580/2433]	eta 0:25:50 lr 0.000000	 wd 0.0500	time 0.8389 (0.8365)	loss 0.1631 (0.4252)	grad_norm 1.3773 (3.3943)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:56:57 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][590/2433]	eta 0:25:41 lr 0.000000	 wd 0.0500	time 0.8422 (0.8366)	loss 0.2281 (0.4213)	grad_norm 1.2352 (3.3557)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][600/2433]	eta 0:25:33 lr 0.000000	 wd 0.0500	time 0.8371 (0.8366)	loss 0.1762 (0.4175)	grad_norm 0.9388 (3.3173)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][610/2433]	eta 0:25:25 lr 0.000000	 wd 0.0500	time 0.8402 (0.8367)	loss 0.2150 (0.4136)	grad_norm 0.9247 (3.2803)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][620/2433]	eta 0:25:17 lr 0.000000	 wd 0.0500	time 0.8371 (0.8367)	loss 0.2142 (0.4100)	grad_norm 1.0072 (3.2434)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][630/2433]	eta 0:25:08 lr 0.000000	 wd 0.0500	time 0.8425 (0.8368)	loss 0.1579 (0.4063)	grad_norm 1.0243 (3.2082)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][640/2433]	eta 0:25:00 lr 0.000000	 wd 0.0500	time 0.8395 (0.8369)	loss 0.1918 (0.4027)	grad_norm 1.0936 (3.1741)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][650/2433]	eta 0:24:52 lr 0.000000	 wd 0.0500	time 0.8433 (0.8369)	loss 0.1903 (0.3992)	grad_norm 0.8456 (3.1398)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:57:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][660/2433]	eta 0:24:43 lr 0.000000	 wd 0.0500	time 0.8382 (0.8370)	loss 0.1675 (0.3962)	grad_norm 1.0130 (3.1072)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][670/2433]	eta 0:24:35 lr 0.000000	 wd 0.0500	time 0.8349 (0.8370)	loss 0.1645 (0.3931)	grad_norm 0.9376 (3.0741)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][680/2433]	eta 0:24:27 lr 0.000000	 wd 0.0500	time 0.8429 (0.8371)	loss 0.1678 (0.3899)	grad_norm 0.9725 (3.0426)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][690/2433]	eta 0:24:19 lr 0.000000	 wd 0.0500	time 0.8430 (0.8371)	loss 0.1632 (0.3867)	grad_norm 0.7714 (3.0109)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][700/2433]	eta 0:24:10 lr 0.000000	 wd 0.0500	time 0.8371 (0.8371)	loss 0.1877 (0.3838)	grad_norm 0.8995 (2.9809)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][710/2433]	eta 0:24:02 lr 0.000000	 wd 0.0500	time 0.8437 (0.8372)	loss 0.2168 (0.3810)	grad_norm 0.8062 (2.9510)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][720/2433]	eta 0:23:54 lr 0.000000	 wd 0.0500	time 0.8441 (0.8372)	loss 0.1274 (0.3782)	grad_norm 0.8067 (2.9214)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:58:55 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][730/2433]	eta 0:23:45 lr 0.000000	 wd 0.0500	time 0.8346 (0.8373)	loss 0.1746 (0.3754)	grad_norm 0.8805 (2.8931)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][740/2433]	eta 0:23:37 lr 0.000001	 wd 0.0500	time 0.8382 (0.8373)	loss 0.1486 (0.3728)	grad_norm 0.8140 (2.8651)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:12 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][750/2433]	eta 0:23:29 lr 0.000001	 wd 0.0500	time 0.8365 (0.8373)	loss 0.1713 (0.3703)	grad_norm 0.7993 (2.8380)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][760/2433]	eta 0:23:20 lr 0.000001	 wd 0.0500	time 0.8406 (0.8374)	loss 0.1728 (0.3677)	grad_norm 0.8141 (2.8118)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][770/2433]	eta 0:23:12 lr 0.000001	 wd 0.0500	time 0.8438 (0.8374)	loss 0.1874 (0.3651)	grad_norm 0.6831 (2.7860)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:37 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][780/2433]	eta 0:23:04 lr 0.000001	 wd 0.0500	time 0.8444 (0.8374)	loss 0.1765 (0.3625)	grad_norm 0.8240 (2.7609)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][790/2433]	eta 0:22:55 lr 0.000001	 wd 0.0500	time 0.8408 (0.8375)	loss 0.2540 (0.3603)	grad_norm 1.0512 (2.7371)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 17:59:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][800/2433]	eta 0:22:47 lr 0.000001	 wd 0.0500	time 0.8456 (0.8375)	loss 0.1535 (0.3580)	grad_norm 0.8572 (2.7130)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:02 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][810/2433]	eta 0:22:39 lr 0.000001	 wd 0.0500	time 0.8445 (0.8376)	loss 0.1822 (0.3557)	grad_norm 0.7571 (2.6895)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][820/2433]	eta 0:22:31 lr 0.000001	 wd 0.0500	time 0.8397 (0.8376)	loss 0.2150 (0.3534)	grad_norm 1.0124 (2.6664)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][830/2433]	eta 0:22:22 lr 0.000001	 wd 0.0500	time 0.8357 (0.8376)	loss 0.1905 (0.3513)	grad_norm 0.7973 (2.6439)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][840/2433]	eta 0:22:14 lr 0.000001	 wd 0.0500	time 0.8458 (0.8377)	loss 0.1882 (0.3492)	grad_norm 0.8399 (2.6229)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][850/2433]	eta 0:22:06 lr 0.000001	 wd 0.0500	time 0.8361 (0.8377)	loss 0.1657 (0.3472)	grad_norm 0.6609 (2.6013)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:44 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][860/2433]	eta 0:21:57 lr 0.000001	 wd 0.0500	time 0.8479 (0.8377)	loss 0.1876 (0.3451)	grad_norm 0.8591 (2.5799)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:00:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][870/2433]	eta 0:21:49 lr 0.000001	 wd 0.0500	time 0.8354 (0.8378)	loss 0.1255 (0.3430)	grad_norm 0.6761 (2.5592)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][880/2433]	eta 0:21:41 lr 0.000001	 wd 0.0500	time 0.8445 (0.8378)	loss 0.1381 (0.3410)	grad_norm 0.6243 (2.5395)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][890/2433]	eta 0:21:32 lr 0.000001	 wd 0.0500	time 0.8419 (0.8378)	loss 0.1589 (0.3391)	grad_norm 0.8108 (2.5200)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][900/2433]	eta 0:21:24 lr 0.000001	 wd 0.0500	time 0.8388 (0.8378)	loss 0.1844 (0.3372)	grad_norm 0.8428 (2.5012)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][910/2433]	eta 0:21:16 lr 0.000001	 wd 0.0500	time 0.8378 (0.8379)	loss 0.1946 (0.3354)	grad_norm 0.7621 (2.4821)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][920/2433]	eta 0:21:07 lr 0.000001	 wd 0.0500	time 0.8420 (0.8379)	loss 0.1619 (0.3338)	grad_norm 0.9274 (2.4644)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][930/2433]	eta 0:20:59 lr 0.000001	 wd 0.0500	time 0.8400 (0.8379)	loss 0.1677 (0.3320)	grad_norm 0.7122 (2.4460)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:01:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][940/2433]	eta 0:20:51 lr 0.000001	 wd 0.0500	time 0.8428 (0.8379)	loss 0.2059 (0.3305)	grad_norm 0.9304 (2.4286)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][950/2433]	eta 0:20:42 lr 0.000001	 wd 0.0500	time 0.8392 (0.8380)	loss 0.1699 (0.3288)	grad_norm 0.7259 (2.4110)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][960/2433]	eta 0:20:34 lr 0.000001	 wd 0.0500	time 0.8375 (0.8380)	loss 0.1496 (0.3272)	grad_norm 0.6504 (2.3938)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:17 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][970/2433]	eta 0:20:26 lr 0.000001	 wd 0.0500	time 0.8422 (0.8380)	loss 0.1600 (0.3256)	grad_norm 0.6718 (2.3771)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:25 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][980/2433]	eta 0:20:17 lr 0.000001	 wd 0.0500	time 0.8434 (0.8380)	loss 0.1660 (0.3241)	grad_norm 0.9126 (2.3608)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:34 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][990/2433]	eta 0:20:09 lr 0.000001	 wd 0.0500	time 0.8404 (0.8381)	loss 0.1756 (0.3225)	grad_norm 0.7371 (2.3446)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1000/2433]	eta 0:20:00 lr 0.000001	 wd 0.0500	time 0.8429 (0.8381)	loss 0.2508 (0.3210)	grad_norm 1.1342 (2.3294)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:50 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1010/2433]	eta 0:19:52 lr 0.000001	 wd 0.0500	time 0.8409 (0.8381)	loss 0.1476 (0.3194)	grad_norm 0.5704 (2.3144)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:02:59 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1020/2433]	eta 0:19:44 lr 0.000001	 wd 0.0500	time 0.8401 (0.8381)	loss 0.1140 (0.3179)	grad_norm 0.7012 (2.2990)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:07 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1030/2433]	eta 0:19:35 lr 0.000001	 wd 0.0500	time 0.8425 (0.8381)	loss 0.2134 (0.3164)	grad_norm 0.8053 (2.2842)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1040/2433]	eta 0:19:27 lr 0.000001	 wd 0.0500	time 0.8402 (0.8381)	loss 0.1534 (0.3150)	grad_norm 0.5856 (2.2693)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1050/2433]	eta 0:19:19 lr 0.000001	 wd 0.0500	time 0.8393 (0.8382)	loss 0.1545 (0.3135)	grad_norm 0.6368 (2.2546)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1060/2433]	eta 0:19:10 lr 0.000001	 wd 0.0500	time 0.8382 (0.8382)	loss 0.1722 (0.3121)	grad_norm 0.7592 (2.2404)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1070/2433]	eta 0:19:02 lr 0.000001	 wd 0.0500	time 0.8429 (0.8382)	loss 0.1835 (0.3107)	grad_norm 1.0968 (2.2267)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1080/2433]	eta 0:18:54 lr 0.000001	 wd 0.0500	time 0.8378 (0.8382)	loss 0.2145 (0.3096)	grad_norm 0.7955 (2.2132)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:03:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1090/2433]	eta 0:18:45 lr 0.000001	 wd 0.0500	time 0.8387 (0.8382)	loss 0.1392 (0.3083)	grad_norm 0.6141 (2.2001)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1100/2433]	eta 0:18:37 lr 0.000001	 wd 0.0500	time 0.8375 (0.8382)	loss 0.1376 (0.3070)	grad_norm 0.8198 (2.1867)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1110/2433]	eta 0:18:29 lr 0.000001	 wd 0.0500	time 0.8382 (0.8383)	loss 0.1554 (0.3059)	grad_norm 0.6749 (2.1742)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1120/2433]	eta 0:18:20 lr 0.000001	 wd 0.0500	time 0.8431 (0.8383)	loss 0.2026 (0.3047)	grad_norm 0.7405 (2.1612)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1130/2433]	eta 0:18:12 lr 0.000001	 wd 0.0500	time 0.8409 (0.8383)	loss 0.1633 (0.3036)	grad_norm 0.6187 (2.1485)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1140/2433]	eta 0:18:03 lr 0.000001	 wd 0.0500	time 0.8428 (0.8383)	loss 0.1546 (0.3023)	grad_norm 0.5752 (2.1367)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1150/2433]	eta 0:17:55 lr 0.000001	 wd 0.0500	time 0.8397 (0.8383)	loss 0.1960 (0.3013)	grad_norm 0.7363 (2.1254)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:04:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1160/2433]	eta 0:17:47 lr 0.000001	 wd 0.0500	time 0.8401 (0.8383)	loss 0.1230 (0.3001)	grad_norm 0.6106 (2.1131)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1170/2433]	eta 0:17:38 lr 0.000001	 wd 0.0500	time 0.8413 (0.8384)	loss 0.1580 (0.2990)	grad_norm 0.6941 (2.1013)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1180/2433]	eta 0:17:30 lr 0.000001	 wd 0.0500	time 0.8389 (0.8384)	loss 0.2036 (0.2979)	grad_norm 0.9501 (2.0902)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1190/2433]	eta 0:17:22 lr 0.000001	 wd 0.0500	time 0.8450 (0.8384)	loss 0.1744 (0.2968)	grad_norm 0.6293 (2.0782)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1200/2433]	eta 0:17:13 lr 0.000001	 wd 0.0500	time 0.8408 (0.8384)	loss 0.1723 (0.2957)	grad_norm 0.8313 (2.0667)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1210/2433]	eta 0:17:05 lr 0.000001	 wd 0.0500	time 0.8423 (0.8384)	loss 0.1483 (0.2945)	grad_norm 0.6509 (2.0556)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1220/2433]	eta 0:16:57 lr 0.000001	 wd 0.0500	time 0.8392 (0.8384)	loss 0.1693 (0.2935)	grad_norm 0.7551 (2.0452)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:05:55 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1230/2433]	eta 0:16:48 lr 0.000001	 wd 0.0500	time 0.8402 (0.8385)	loss 0.1977 (0.2925)	grad_norm 0.9202 (2.0345)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1240/2433]	eta 0:16:40 lr 0.000001	 wd 0.0500	time 0.8391 (0.8385)	loss 0.2868 (0.2915)	grad_norm 1.1011 (2.0245)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:12 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1250/2433]	eta 0:16:31 lr 0.000001	 wd 0.0500	time 0.8400 (0.8385)	loss 0.2205 (0.2906)	grad_norm 0.7707 (2.0145)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1260/2433]	eta 0:16:23 lr 0.000001	 wd 0.0500	time 0.8395 (0.8385)	loss 0.1329 (0.2895)	grad_norm 0.7792 (2.0043)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1270/2433]	eta 0:16:15 lr 0.000001	 wd 0.0500	time 0.8403 (0.8385)	loss 0.1946 (0.2886)	grad_norm 0.8993 (1.9943)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:37 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1280/2433]	eta 0:16:06 lr 0.000001	 wd 0.0500	time 0.8386 (0.8385)	loss 0.1720 (0.2877)	grad_norm 0.6043 (1.9846)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1290/2433]	eta 0:15:58 lr 0.000001	 wd 0.0500	time 0.8388 (0.8385)	loss 0.1165 (0.2867)	grad_norm 0.7535 (1.9748)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:06:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1300/2433]	eta 0:15:50 lr 0.000001	 wd 0.0500	time 0.8450 (0.8385)	loss 0.1340 (0.2857)	grad_norm 0.5669 (1.9649)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:02 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1310/2433]	eta 0:15:41 lr 0.000001	 wd 0.0500	time 0.8427 (0.8385)	loss 0.1650 (0.2848)	grad_norm 0.6260 (1.9550)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1320/2433]	eta 0:15:33 lr 0.000001	 wd 0.0500	time 0.8399 (0.8386)	loss 0.1470 (0.2839)	grad_norm 0.6415 (1.9455)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1330/2433]	eta 0:15:24 lr 0.000001	 wd 0.0500	time 0.8416 (0.8386)	loss 0.1817 (0.2832)	grad_norm 0.6901 (1.9365)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1340/2433]	eta 0:15:16 lr 0.000001	 wd 0.0500	time 0.8405 (0.8386)	loss 0.1194 (0.2821)	grad_norm 0.5703 (1.9271)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1350/2433]	eta 0:15:08 lr 0.000001	 wd 0.0500	time 0.8404 (0.8386)	loss 0.1075 (0.2813)	grad_norm 0.5364 (1.9181)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:44 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1360/2433]	eta 0:14:59 lr 0.000001	 wd 0.0500	time 0.8395 (0.8386)	loss 0.1801 (0.2804)	grad_norm 0.6631 (1.9094)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:07:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1370/2433]	eta 0:14:51 lr 0.000001	 wd 0.0500	time 0.8387 (0.8386)	loss 0.1825 (0.2796)	grad_norm 0.6142 (1.9010)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1380/2433]	eta 0:14:43 lr 0.000001	 wd 0.0500	time 0.8385 (0.8386)	loss 0.1225 (0.2787)	grad_norm 0.8016 (1.8927)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1390/2433]	eta 0:14:34 lr 0.000001	 wd 0.0500	time 0.8397 (0.8386)	loss 0.1657 (0.2780)	grad_norm 0.6360 (1.8845)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1400/2433]	eta 0:14:26 lr 0.000001	 wd 0.0500	time 0.8354 (0.8386)	loss 0.1957 (0.2773)	grad_norm 0.7749 (1.8761)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1410/2433]	eta 0:14:17 lr 0.000001	 wd 0.0500	time 0.8387 (0.8387)	loss 0.1578 (0.2766)	grad_norm 0.6660 (1.8681)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1420/2433]	eta 0:14:09 lr 0.000001	 wd 0.0500	time 0.8419 (0.8387)	loss 0.1439 (0.2756)	grad_norm 0.6465 (1.8596)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1430/2433]	eta 0:14:01 lr 0.000001	 wd 0.0500	time 0.8409 (0.8387)	loss 0.2086 (0.2749)	grad_norm 0.9704 (1.8522)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:08:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1440/2433]	eta 0:13:52 lr 0.000001	 wd 0.0500	time 0.8386 (0.8387)	loss 0.1861 (0.2742)	grad_norm 0.7729 (1.8448)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1450/2433]	eta 0:13:44 lr 0.000001	 wd 0.0500	time 0.8388 (0.8387)	loss 0.1663 (0.2735)	grad_norm 0.7372 (1.8372)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1460/2433]	eta 0:13:36 lr 0.000001	 wd 0.0500	time 0.8421 (0.8387)	loss 0.1124 (0.2727)	grad_norm 1.0642 (1.8301)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:17 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1470/2433]	eta 0:13:27 lr 0.000001	 wd 0.0500	time 0.8396 (0.8387)	loss 0.1512 (0.2720)	grad_norm 0.6295 (1.8228)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:25 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1480/2433]	eta 0:13:19 lr 0.000001	 wd 0.0500	time 0.8415 (0.8387)	loss 0.1978 (0.2713)	grad_norm 0.7576 (1.8153)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:34 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1490/2433]	eta 0:13:10 lr 0.000001	 wd 0.0500	time 0.8367 (0.8387)	loss 0.1962 (0.2706)	grad_norm 0.8024 (1.8078)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1500/2433]	eta 0:13:02 lr 0.000001	 wd 0.0500	time 0.8438 (0.8387)	loss 0.1716 (0.2698)	grad_norm 0.7118 (1.8008)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:50 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1510/2433]	eta 0:12:54 lr 0.000001	 wd 0.0500	time 0.8429 (0.8387)	loss 0.2193 (0.2691)	grad_norm 0.6906 (1.7934)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:09:59 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1520/2433]	eta 0:12:45 lr 0.000001	 wd 0.0500	time 0.8415 (0.8387)	loss 0.2678 (0.2685)	grad_norm 1.1537 (1.7865)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:07 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1530/2433]	eta 0:12:37 lr 0.000001	 wd 0.0500	time 0.8353 (0.8388)	loss 0.1514 (0.2678)	grad_norm 0.6001 (1.7797)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1540/2433]	eta 0:12:29 lr 0.000001	 wd 0.0500	time 0.8399 (0.8388)	loss 0.1259 (0.2672)	grad_norm 0.5380 (1.7727)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1550/2433]	eta 0:12:20 lr 0.000001	 wd 0.0500	time 0.8434 (0.8388)	loss 0.1472 (0.2666)	grad_norm 0.6614 (1.7666)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1560/2433]	eta 0:12:12 lr 0.000001	 wd 0.0500	time 0.8378 (0.8388)	loss 0.2221 (0.2660)	grad_norm 0.8624 (1.7598)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1570/2433]	eta 0:12:03 lr 0.000001	 wd 0.0500	time 0.8392 (0.8388)	loss 0.1317 (0.2653)	grad_norm 0.6377 (1.7530)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1580/2433]	eta 0:11:55 lr 0.000001	 wd 0.0500	time 0.8435 (0.8388)	loss 0.1597 (0.2646)	grad_norm 0.9407 (1.7464)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:10:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1590/2433]	eta 0:11:47 lr 0.000001	 wd 0.0500	time 0.8399 (0.8388)	loss 0.2365 (0.2638)	grad_norm 1.0466 (1.7395)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1600/2433]	eta 0:11:38 lr 0.000001	 wd 0.0500	time 0.8381 (0.8388)	loss 0.1305 (0.2632)	grad_norm 0.7992 (1.7335)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1610/2433]	eta 0:11:30 lr 0.000001	 wd 0.0500	time 0.8441 (0.8388)	loss 0.1606 (0.2626)	grad_norm 0.5399 (1.7271)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1620/2433]	eta 0:11:21 lr 0.000001	 wd 0.0500	time 0.8393 (0.8388)	loss 0.2406 (0.2621)	grad_norm 0.8191 (1.7210)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1630/2433]	eta 0:11:13 lr 0.000001	 wd 0.0500	time 0.8386 (0.8388)	loss 0.1417 (0.2614)	grad_norm 0.5528 (1.7146)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1640/2433]	eta 0:11:05 lr 0.000001	 wd 0.0500	time 0.8402 (0.8388)	loss 0.1837 (0.2608)	grad_norm 0.7353 (1.7085)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1650/2433]	eta 0:10:56 lr 0.000001	 wd 0.0500	time 0.8425 (0.8388)	loss 0.1937 (0.2602)	grad_norm 0.9741 (1.7024)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:11:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1660/2433]	eta 0:10:48 lr 0.000001	 wd 0.0500	time 0.8378 (0.8389)	loss 0.1794 (0.2595)	grad_norm 0.6491 (1.6962)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1670/2433]	eta 0:10:40 lr 0.000001	 wd 0.0500	time 0.8391 (0.8389)	loss 0.2166 (0.2589)	grad_norm 0.8563 (1.6902)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1680/2433]	eta 0:10:31 lr 0.000001	 wd 0.0500	time 0.8392 (0.8389)	loss 0.1424 (0.2583)	grad_norm 0.5961 (1.6843)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1690/2433]	eta 0:10:23 lr 0.000001	 wd 0.0500	time 0.8408 (0.8389)	loss 0.1955 (0.2578)	grad_norm 0.9534 (1.6783)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1700/2433]	eta 0:10:14 lr 0.000001	 wd 0.0500	time 0.8381 (0.8389)	loss 0.1536 (0.2572)	grad_norm 0.6240 (1.6727)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1710/2433]	eta 0:10:06 lr 0.000001	 wd 0.0500	time 0.8414 (0.8389)	loss 0.1327 (0.2566)	grad_norm 0.6859 (1.6670)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1720/2433]	eta 0:09:58 lr 0.000001	 wd 0.0500	time 0.8409 (0.8389)	loss 0.2051 (0.2561)	grad_norm 1.1608 (1.6618)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:12:55 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1730/2433]	eta 0:09:49 lr 0.000001	 wd 0.0500	time 0.8393 (0.8389)	loss 0.1894 (0.2555)	grad_norm 0.5710 (1.6559)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1740/2433]	eta 0:09:41 lr 0.000001	 wd 0.0500	time 0.8407 (0.8389)	loss 0.1322 (0.2550)	grad_norm 0.6708 (1.6503)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:12 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1750/2433]	eta 0:09:32 lr 0.000001	 wd 0.0500	time 0.8396 (0.8389)	loss 0.2110 (0.2545)	grad_norm 1.0052 (1.6451)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1760/2433]	eta 0:09:24 lr 0.000001	 wd 0.0500	time 0.8381 (0.8389)	loss 0.1331 (0.2540)	grad_norm 0.7268 (1.6396)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1770/2433]	eta 0:09:16 lr 0.000001	 wd 0.0500	time 0.8398 (0.8389)	loss 0.1346 (0.2534)	grad_norm 0.5335 (1.6344)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:37 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1780/2433]	eta 0:09:07 lr 0.000001	 wd 0.0500	time 0.8417 (0.8390)	loss 0.1638 (0.2529)	grad_norm 0.6165 (1.6293)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1790/2433]	eta 0:08:59 lr 0.000001	 wd 0.0500	time 0.8389 (0.8390)	loss 0.1129 (0.2524)	grad_norm 0.6022 (1.6243)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:13:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1800/2433]	eta 0:08:51 lr 0.000001	 wd 0.0500	time 0.8383 (0.8390)	loss 0.1166 (0.2519)	grad_norm 0.7417 (1.6194)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:02 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1810/2433]	eta 0:08:42 lr 0.000001	 wd 0.0500	time 0.8441 (0.8390)	loss 0.2188 (0.2514)	grad_norm 0.7126 (1.6142)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1820/2433]	eta 0:08:34 lr 0.000001	 wd 0.0500	time 0.8395 (0.8390)	loss 0.1886 (0.2511)	grad_norm 0.9391 (1.6099)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1830/2433]	eta 0:08:25 lr 0.000001	 wd 0.0500	time 0.8390 (0.8390)	loss 0.1590 (0.2505)	grad_norm 0.8083 (1.6051)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1840/2433]	eta 0:08:17 lr 0.000001	 wd 0.0500	time 0.8390 (0.8390)	loss 0.2036 (0.2501)	grad_norm 0.9324 (1.6005)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1850/2433]	eta 0:08:09 lr 0.000001	 wd 0.0500	time 0.8396 (0.8390)	loss 0.1361 (0.2496)	grad_norm 0.6622 (1.5955)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:44 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1860/2433]	eta 0:08:00 lr 0.000001	 wd 0.0500	time 0.8385 (0.8390)	loss 0.1591 (0.2491)	grad_norm 0.9460 (1.5908)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:14:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1870/2433]	eta 0:07:52 lr 0.000001	 wd 0.0500	time 0.8419 (0.8390)	loss 0.0995 (0.2487)	grad_norm 0.6729 (1.5862)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1880/2433]	eta 0:07:43 lr 0.000001	 wd 0.0500	time 0.8494 (0.8390)	loss 0.1428 (0.2481)	grad_norm 0.5648 (1.5816)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1890/2433]	eta 0:07:35 lr 0.000001	 wd 0.0500	time 0.8388 (0.8390)	loss 0.1248 (0.2476)	grad_norm 0.7040 (1.5764)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1900/2433]	eta 0:07:27 lr 0.000001	 wd 0.0500	time 0.8380 (0.8390)	loss 0.1394 (0.2472)	grad_norm 0.7458 (1.5721)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1910/2433]	eta 0:07:18 lr 0.000001	 wd 0.0500	time 0.8384 (0.8390)	loss 0.1859 (0.2467)	grad_norm 0.9584 (1.5676)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1920/2433]	eta 0:07:10 lr 0.000001	 wd 0.0500	time 0.8387 (0.8390)	loss 0.1972 (0.2464)	grad_norm 0.8249 (1.5636)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1930/2433]	eta 0:07:02 lr 0.000001	 wd 0.0500	time 0.8453 (0.8390)	loss 0.1228 (0.2460)	grad_norm 0.5858 (1.5591)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:15:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1940/2433]	eta 0:06:53 lr 0.000001	 wd 0.0500	time 0.8383 (0.8390)	loss 0.1238 (0.2455)	grad_norm 0.6633 (1.5548)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:16:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1950/2433]	eta 0:06:45 lr 0.000001	 wd 0.0500	time 0.8361 (0.8390)	loss 0.1246 (0.2451)	grad_norm 0.7013 (1.5506)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:16:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1960/2433]	eta 0:06:36 lr 0.000001	 wd 0.0500	time 0.8406 (0.8391)	loss 0.1499 (0.2446)	grad_norm 0.6646 (1.5462)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:16:17 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1970/2433]	eta 0:06:28 lr 0.000001	 wd 0.0500	time 0.8401 (0.8391)	loss 0.1857 (0.2443)	grad_norm 0.7958 (1.5424)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:16:25 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1980/2433]	eta 0:06:20 lr 0.000001	 wd 0.0500	time 0.8428 (0.8391)	loss 0.1206 (0.2439)	grad_norm 0.7463 (1.5385)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:16:34 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1990/2433]	eta 0:06:11 lr 0.000001	 wd 0.0500	time 0.8449 (0.8391)	loss 0.1607 (0.2436)	grad_norm 0.6485 (1.5344)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:16:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2000/2433]	eta 0:06:03 lr 0.000001	 wd 0.0500	time 0.8391 (0.8391)	loss 0.2073 (0.2431)	grad_norm 0.9031 (1.5306)	loss_scale 131072.0000 (65601.5032)	mem 6085MB
[2024-03-21 18:16:50 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2010/2433]	eta 0:05:54 lr 0.000001	 wd 0.0500	time 0.8447 (0.8391)	loss 0.1987 (0.2428)	grad_norm 0.9762 (1.5269)	loss_scale 131072.0000 (65927.0651)	mem 6085MB
[2024-03-21 18:16:59 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2020/2433]	eta 0:05:46 lr 0.000001	 wd 0.0500	time 0.8385 (0.8391)	loss 0.1806 (0.2425)	grad_norm 0.7254 (1.5234)	loss_scale 131072.0000 (66249.4052)	mem 6085MB
[2024-03-21 18:17:07 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2030/2433]	eta 0:05:38 lr 0.000001	 wd 0.0500	time 0.8427 (0.8391)	loss 0.1742 (0.2421)	grad_norm 0.6540 (1.5191)	loss_scale 131072.0000 (66568.5711)	mem 6085MB
[2024-03-21 18:17:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2040/2433]	eta 0:05:29 lr 0.000001	 wd 0.0500	time 0.8411 (0.8391)	loss 0.1756 (0.2417)	grad_norm 0.6166 (1.5150)	loss_scale 131072.0000 (66884.6095)	mem 6085MB
[2024-03-21 18:17:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2050/2433]	eta 0:05:21 lr 0.000001	 wd 0.0500	time 0.8384 (0.8391)	loss 0.1783 (0.2414)	grad_norm 0.7308 (1.5112)	loss_scale 131072.0000 (67197.5661)	mem 6085MB
[2024-03-21 18:17:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2060/2433]	eta 0:05:12 lr 0.000001	 wd 0.0500	time 0.8387 (0.8391)	loss 0.1767 (0.2410)	grad_norm 0.9379 (1.5075)	loss_scale 131072.0000 (67507.4857)	mem 6085MB
[2024-03-21 18:17:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2070/2433]	eta 0:05:04 lr 0.000001	 wd 0.0500	time 0.8352 (0.8391)	loss 0.1653 (0.2406)	grad_norm 0.8204 (1.5039)	loss_scale 131072.0000 (67814.4124)	mem 6085MB
[2024-03-21 18:17:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2080/2433]	eta 0:04:56 lr 0.000001	 wd 0.0500	time 0.8386 (0.8391)	loss 0.1491 (0.2403)	grad_norm 0.9391 (1.5005)	loss_scale 131072.0000 (68118.3892)	mem 6085MB
[2024-03-21 18:17:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2090/2433]	eta 0:04:47 lr 0.000001	 wd 0.0500	time 0.8387 (0.8391)	loss 0.1647 (0.2400)	grad_norm 0.5924 (1.4969)	loss_scale 131072.0000 (68419.4586)	mem 6085MB
[2024-03-21 18:18:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2100/2433]	eta 0:04:39 lr 0.000001	 wd 0.0500	time 0.8394 (0.8391)	loss 0.1775 (0.2397)	grad_norm 0.7533 (1.4937)	loss_scale 131072.0000 (68717.6621)	mem 6085MB
[2024-03-21 18:18:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2110/2433]	eta 0:04:31 lr 0.000001	 wd 0.0500	time 0.8423 (0.8391)	loss 0.1369 (0.2393)	grad_norm 0.8893 (1.4900)	loss_scale 131072.0000 (69013.0403)	mem 6085MB
[2024-03-21 18:18:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2120/2433]	eta 0:04:22 lr 0.000001	 wd 0.0500	time 0.8403 (0.8391)	loss 0.1600 (0.2389)	grad_norm 0.7446 (1.4864)	loss_scale 131072.0000 (69305.6332)	mem 6085MB
[2024-03-21 18:18:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2130/2433]	eta 0:04:14 lr 0.000001	 wd 0.0500	time 0.8417 (0.8391)	loss 0.1406 (0.2385)	grad_norm 0.5236 (1.4828)	loss_scale 131072.0000 (69595.4801)	mem 6085MB
[2024-03-21 18:18:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2140/2433]	eta 0:04:05 lr 0.000001	 wd 0.0500	time 0.8404 (0.8391)	loss 0.1911 (0.2381)	grad_norm 1.1287 (1.4794)	loss_scale 131072.0000 (69882.6193)	mem 6085MB
[2024-03-21 18:18:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2150/2433]	eta 0:03:57 lr 0.000001	 wd 0.0500	time 0.8456 (0.8391)	loss 0.1272 (0.2378)	grad_norm 0.5719 (1.4756)	loss_scale 131072.0000 (70167.0888)	mem 6085MB
[2024-03-21 18:18:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2160/2433]	eta 0:03:49 lr 0.000001	 wd 0.0500	time 0.8382 (0.8391)	loss 0.1534 (0.2374)	grad_norm 0.6979 (1.4722)	loss_scale 131072.0000 (70448.9255)	mem 6085MB
[2024-03-21 18:19:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2170/2433]	eta 0:03:40 lr 0.000001	 wd 0.0500	time 0.8426 (0.8392)	loss 0.1424 (0.2370)	grad_norm 0.6705 (1.4689)	loss_scale 131072.0000 (70728.1658)	mem 6085MB
[2024-03-21 18:19:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2180/2433]	eta 0:03:32 lr 0.000001	 wd 0.0500	time 0.8413 (0.8392)	loss 0.1353 (0.2368)	grad_norm 0.8926 (1.4661)	loss_scale 131072.0000 (71004.8455)	mem 6085MB
[2024-03-21 18:19:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2190/2433]	eta 0:03:23 lr 0.000001	 wd 0.0500	time 0.8383 (0.8392)	loss 0.1471 (0.2364)	grad_norm 0.7061 (1.4628)	loss_scale 131072.0000 (71278.9995)	mem 6085MB
[2024-03-21 18:19:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2200/2433]	eta 0:03:15 lr 0.000001	 wd 0.0500	time 0.8405 (0.8392)	loss 0.1821 (0.2361)	grad_norm 1.0349 (1.4597)	loss_scale 131072.0000 (71550.6624)	mem 6085MB
[2024-03-21 18:19:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2210/2433]	eta 0:03:07 lr 0.000001	 wd 0.0500	time 0.8394 (0.8392)	loss 0.1583 (0.2358)	grad_norm 0.6397 (1.4565)	loss_scale 131072.0000 (71819.8679)	mem 6085MB
[2024-03-21 18:19:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2220/2433]	eta 0:02:58 lr 0.000001	 wd 0.0500	time 0.8377 (0.8392)	loss 0.1728 (0.2354)	grad_norm 0.6844 (1.4532)	loss_scale 131072.0000 (72086.6493)	mem 6085MB
[2024-03-21 18:19:55 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2230/2433]	eta 0:02:50 lr 0.000001	 wd 0.0500	time 0.8409 (0.8392)	loss 0.1950 (0.2351)	grad_norm 0.6311 (1.4499)	loss_scale 131072.0000 (72351.0390)	mem 6085MB
[2024-03-21 18:20:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2240/2433]	eta 0:02:41 lr 0.000001	 wd 0.0500	time 0.8375 (0.8392)	loss 0.1629 (0.2347)	grad_norm 0.7563 (1.4466)	loss_scale 131072.0000 (72613.0692)	mem 6085MB
[2024-03-21 18:20:12 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2250/2433]	eta 0:02:33 lr 0.000001	 wd 0.0500	time 0.8400 (0.8392)	loss 0.2454 (0.2345)	grad_norm 0.8575 (1.4435)	loss_scale 131072.0000 (72872.7712)	mem 6085MB
[2024-03-21 18:20:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2260/2433]	eta 0:02:25 lr 0.000001	 wd 0.0500	time 0.8439 (0.8392)	loss 0.0959 (0.2341)	grad_norm 0.6649 (1.4402)	loss_scale 131072.0000 (73130.1760)	mem 6085MB
[2024-03-21 18:20:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2270/2433]	eta 0:02:16 lr 0.000001	 wd 0.0500	time 0.8388 (0.8392)	loss 0.2063 (0.2338)	grad_norm 0.8156 (1.4369)	loss_scale 131072.0000 (73385.3140)	mem 6085MB
[2024-03-21 18:20:37 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2280/2433]	eta 0:02:08 lr 0.000001	 wd 0.0500	time 0.8406 (0.8392)	loss 0.1510 (0.2335)	grad_norm 0.7138 (1.4338)	loss_scale 131072.0000 (73638.2148)	mem 6085MB
[2024-03-21 18:20:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2290/2433]	eta 0:02:00 lr 0.000002	 wd 0.0500	time 0.8413 (0.8392)	loss 0.1913 (0.2331)	grad_norm 0.7831 (1.4308)	loss_scale 131072.0000 (73888.9079)	mem 6085MB
[2024-03-21 18:20:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2300/2433]	eta 0:01:51 lr 0.000002	 wd 0.0500	time 0.8418 (0.8392)	loss 0.1854 (0.2329)	grad_norm 0.7171 (1.4278)	loss_scale 131072.0000 (74137.4220)	mem 6085MB
[2024-03-21 18:21:02 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2310/2433]	eta 0:01:43 lr 0.000002	 wd 0.0500	time 0.8394 (0.8392)	loss 0.0996 (0.2326)	grad_norm 0.8370 (1.4247)	loss_scale 131072.0000 (74383.7854)	mem 6085MB
[2024-03-21 18:21:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2320/2433]	eta 0:01:34 lr 0.000002	 wd 0.0500	time 0.8364 (0.8392)	loss 0.1919 (0.2323)	grad_norm 0.6657 (1.4219)	loss_scale 131072.0000 (74628.0259)	mem 6085MB
[2024-03-21 18:21:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2330/2433]	eta 0:01:26 lr 0.000002	 wd 0.0500	time 0.8369 (0.8392)	loss 0.1703 (0.2319)	grad_norm 0.7023 (1.4189)	loss_scale 131072.0000 (74870.1707)	mem 6085MB
[2024-03-21 18:21:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2340/2433]	eta 0:01:18 lr 0.000002	 wd 0.0500	time 0.8416 (0.8392)	loss 0.1680 (0.2316)	grad_norm 1.0399 (1.4160)	loss_scale 131072.0000 (75110.2469)	mem 6085MB
[2024-03-21 18:21:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2350/2433]	eta 0:01:09 lr 0.000002	 wd 0.0500	time 0.8406 (0.8392)	loss 0.1496 (0.2313)	grad_norm 0.6333 (1.4133)	loss_scale 131072.0000 (75348.2807)	mem 6085MB
[2024-03-21 18:21:44 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2360/2433]	eta 0:01:01 lr 0.000002	 wd 0.0500	time 0.8395 (0.8392)	loss 0.1418 (0.2309)	grad_norm 0.6589 (1.4104)	loss_scale 131072.0000 (75584.2982)	mem 6085MB
[2024-03-21 18:21:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2370/2433]	eta 0:00:52 lr 0.000002	 wd 0.0500	time 0.8390 (0.8392)	loss 0.1967 (0.2307)	grad_norm 0.9802 (1.4079)	loss_scale 131072.0000 (75818.3248)	mem 6085MB
[2024-03-21 18:22:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2380/2433]	eta 0:00:44 lr 0.000002	 wd 0.0500	time 0.8414 (0.8392)	loss 0.1437 (0.2304)	grad_norm 0.6923 (1.4052)	loss_scale 131072.0000 (76050.3856)	mem 6085MB
[2024-03-21 18:22:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2390/2433]	eta 0:00:36 lr 0.000002	 wd 0.0500	time 0.8414 (0.8392)	loss 0.1730 (0.2301)	grad_norm 0.6639 (1.4027)	loss_scale 131072.0000 (76280.5052)	mem 6085MB
[2024-03-21 18:22:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2400/2433]	eta 0:00:27 lr 0.000002	 wd 0.0500	time 0.8366 (0.8392)	loss 0.1352 (0.2299)	grad_norm 0.6531 (1.4000)	loss_scale 131072.0000 (76508.7080)	mem 6085MB
[2024-03-21 18:22:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2410/2433]	eta 0:00:19 lr 0.000002	 wd 0.0500	time 0.8432 (0.8392)	loss 0.1537 (0.2296)	grad_norm 0.6624 (1.3973)	loss_scale 131072.0000 (76735.0178)	mem 6085MB
[2024-03-21 18:22:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2420/2433]	eta 0:00:10 lr 0.000002	 wd 0.0500	time 0.8348 (0.8392)	loss 0.1837 (0.2293)	grad_norm 0.8449 (1.3946)	loss_scale 131072.0000 (76959.4581)	mem 6085MB
[2024-03-21 18:22:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][2430/2433]	eta 0:00:02 lr 0.000002	 wd 0.0500	time 0.8418 (0.8392)	loss 0.1310 (0.2290)	grad_norm 0.5673 (1.3920)	loss_scale 131072.0000 (77182.0518)	mem 6085MB
[2024-03-21 18:22:45 swin_base_patch4_window7_224] (main_classification_ddp.py 426): INFO EPOCH 0 training takes 0:34:01
[2024-03-21 18:32:08 swin_base_patch4_window7_224] (main_classification_ddp.py 603): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 18:32:08 swin_base_patch4_window7_224] (main_classification_ddp.py 606): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.25e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2024-03-21 18:32:08 swin_base_patch4_window7_224] (main_classification_ddp.py 607): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 18:32:13 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 18:32:14 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 18:32:14 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 18:32:14 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 18:32:16 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 18:32:17 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 18:32:17 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 18:33:32 swin_base_patch4_window7_224] (main_classification_ddp.py 603): INFO Full config saved to /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1/config.json
[2024-03-21 18:33:32 swin_base_patch4_window7_224] (main_classification_ddp.py 606): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BACKBONE: swin_base
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  CROP_SIZE: 256
  DATASET: NIHchest
  DATA_PATH: /mnt/dfs/jpang12/datasets/nih_xray14/images/images
  FOLD: '1'
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_test_official.txt
  TRAIN_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_train_official.txt
  VAL_LIST: /mnt/dfs/ssiingh/BenchmarkArk/dataset/Xray14_val_official.txt
  ZIP_MODE: false
DEVICE: '1'
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LINEAR_PROB: false
LOCAL_RANK: 0
MODE: train
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_base_patch4_window7_224
  NUM_CLASSES: 14
  PRETRAINED: /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: /mnt/dfs/ssiingh/ACE/downstream_checkpoints/NIHChestX-ray14/simmim_compose12N_infonce1
PATIENCE: 20
POPAR_FORM: true
PRETRAIN_MODE: simmim_compose12N_infonce
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 3.125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 150
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.125e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.125e-08
  WEIGHT_DECAY: 0.05

[2024-03-21 18:33:32 swin_base_patch4_window7_224] (main_classification_ddp.py 607): INFO {"cfg": "configs/swin/swin_base_patch4_window7_224.yaml", "opts": null, "backbone": "swin_base", "batch_size": 32, "dataset": "NIHchest", "img_size": 224, "pretrain_mode": "simmim_compose12N_infonce", "fold": "1", "pretrain_weight": "/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth", "mode": "train", "zip": false, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null, "master_port": "12345", "linear_prob": false, "patience": 20}
[2024-03-21 18:33:37 swin_base_patch4_window7_224] (main_classification_ddp.py 119): INFO Creating model:swin/swin_base_patch4_window7_224
[2024-03-21 18:33:38 swin_base_patch4_window7_224] (main_classification_ddp.py 130): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.152)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.174)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.217)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.239)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.283)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.304)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.326)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.348)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.370)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.413)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.435)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.457)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.478)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
[2024-03-21 18:33:38 swin_base_patch4_window7_224] (main_classification_ddp.py 133): INFO number of params: 86757574
[2024-03-21 18:33:38 swin_base_patch4_window7_224] (main_classification_ddp.py 136): INFO number of GFLOPs: 15.437463552
[2024-03-21 18:33:40 swin_base_patch4_window7_224] (utils.py 48): INFO ==============> Loading weight /mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth for fine-tuning......
[2024-03-21 18:33:41 swin_base_patch4_window7_224] (utils.py 283): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.17.attn_mask', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.2.weight', 'module.head.mlp.2.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.last_layer.weight_g', 'module.head.last_layer.weight_v', 'module.DenseHead.layers.0.0.weight', 'module.DenseHead.layers.0.0.bias', 'module.DenseHead.layers.0.1.weight', 'module.DenseHead.layers.0.1.bias', 'module.DenseHead.layers.0.1.running_mean', 'module.DenseHead.layers.0.1.running_var', 'module.DenseHead.layers.0.1.num_batches_tracked', 'module.DenseHead.layers.1.0.weight', 'module.DenseHead.layers.1.0.bias', 'module.DenseHead.layers.1.1.weight', 'module.DenseHead.layers.1.1.bias', 'module.DenseHead.layers.1.1.running_mean', 'module.DenseHead.layers.1.1.running_var', 'module.DenseHead.layers.1.1.num_batches_tracked', 'module.DenseHead.layers.2.0.weight', 'module.DenseHead.layers.2.0.bias', 'module.DenseHead.layers.2.1.weight', 'module.DenseHead.layers.2.1.bias', 'module.DenseHead.layers.2.1.running_mean', 'module.DenseHead.layers.2.1.running_var', 'module.DenseHead.layers.2.1.num_batches_tracked', 'module.predictor_.0.weight', 'module.predictor_.1.weight', 'module.predictor_.1.bias', 'module.predictor_.3.weight', 'module.predictor_.3.bias'])
[2024-03-21 18:33:41 swin_base_patch4_window7_224] (utils.py 285): INFO => loaded successfully '/mnt/dfs/ssiingh/ACE/models/ACE_contrast_12n_global_inequal_swinb.pth'
[2024-03-21 18:33:41 swin_base_patch4_window7_224] (main_classification_ddp.py 217): INFO Start training
[2024-03-21 18:33:44 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][0/2433]	eta 2:14:14 lr 0.000000	 wd 0.0500	time 3.3107 (3.3107)	loss 0.7257 (0.7257)	grad_norm 5.4999 (5.4999)	loss_scale 65536.0000 (65536.0000)	mem 5410MB
[2024-03-21 18:33:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][10/2433]	eta 0:41:28 lr 0.000000	 wd 0.0500	time 0.8003 (1.0271)	loss 0.7592 (0.7504)	grad_norm 5.6230 (5.6001)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][20/2433]	eta 0:36:59 lr 0.000000	 wd 0.0500	time 0.8048 (0.9199)	loss 0.7290 (0.7473)	grad_norm 5.4585 (5.5482)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][30/2433]	eta 0:35:22 lr 0.000000	 wd 0.0500	time 0.8024 (0.8834)	loss 0.7100 (0.7456)	grad_norm 5.8137 (5.5667)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][40/2433]	eta 0:34:29 lr 0.000000	 wd 0.0500	time 0.8094 (0.8650)	loss 0.7255 (0.7423)	grad_norm 5.0632 (5.5297)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][50/2433]	eta 0:33:56 lr 0.000000	 wd 0.0500	time 0.8144 (0.8548)	loss 0.7266 (0.7376)	grad_norm 5.1038 (5.4897)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][60/2433]	eta 0:33:33 lr 0.000000	 wd 0.0500	time 0.8174 (0.8484)	loss 0.7080 (0.7339)	grad_norm 5.4889 (5.4607)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][70/2433]	eta 0:33:15 lr 0.000000	 wd 0.0500	time 0.8215 (0.8444)	loss 0.7078 (0.7307)	grad_norm 5.2353 (5.4426)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][80/2433]	eta 0:32:59 lr 0.000000	 wd 0.0500	time 0.8166 (0.8414)	loss 0.7105 (0.7273)	grad_norm 5.1463 (5.4160)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:34:57 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][90/2433]	eta 0:32:45 lr 0.000000	 wd 0.0500	time 0.8188 (0.8389)	loss 0.6973 (0.7233)	grad_norm 5.1963 (5.3990)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][100/2433]	eta 0:32:33 lr 0.000000	 wd 0.0500	time 0.8186 (0.8372)	loss 0.6843 (0.7189)	grad_norm 5.1930 (5.3708)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][110/2433]	eta 0:32:21 lr 0.000000	 wd 0.0500	time 0.8259 (0.8359)	loss 0.6622 (0.7144)	grad_norm 4.6549 (5.3307)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][120/2433]	eta 0:32:11 lr 0.000000	 wd 0.0500	time 0.8290 (0.8351)	loss 0.6583 (0.7093)	grad_norm 4.7061 (5.2971)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][130/2433]	eta 0:32:01 lr 0.000000	 wd 0.0500	time 0.8275 (0.8344)	loss 0.6183 (0.7043)	grad_norm 4.7097 (5.2600)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][140/2433]	eta 0:31:52 lr 0.000000	 wd 0.0500	time 0.8273 (0.8339)	loss 0.6170 (0.6995)	grad_norm 4.7725 (5.2269)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][150/2433]	eta 0:31:42 lr 0.000000	 wd 0.0500	time 0.8310 (0.8335)	loss 0.6099 (0.6940)	grad_norm 4.8366 (5.1924)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:35:55 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][160/2433]	eta 0:31:33 lr 0.000000	 wd 0.0500	time 0.8292 (0.8332)	loss 0.5943 (0.6884)	grad_norm 4.6827 (5.1633)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:03 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][170/2433]	eta 0:31:24 lr 0.000000	 wd 0.0500	time 0.8291 (0.8329)	loss 0.5807 (0.6831)	grad_norm 4.3875 (5.1260)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][180/2433]	eta 0:31:16 lr 0.000000	 wd 0.0500	time 0.8285 (0.8328)	loss 0.5456 (0.6777)	grad_norm 4.3759 (5.0947)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][190/2433]	eta 0:31:07 lr 0.000000	 wd 0.0500	time 0.8304 (0.8326)	loss 0.5846 (0.6720)	grad_norm 4.1415 (5.0564)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][200/2433]	eta 0:30:59 lr 0.000000	 wd 0.0500	time 0.8323 (0.8326)	loss 0.5316 (0.6662)	grad_norm 4.2442 (5.0170)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][210/2433]	eta 0:30:50 lr 0.000000	 wd 0.0500	time 0.8321 (0.8325)	loss 0.5401 (0.6602)	grad_norm 4.1245 (4.9774)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:45 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][220/2433]	eta 0:30:42 lr 0.000000	 wd 0.0500	time 0.8311 (0.8324)	loss 0.5305 (0.6541)	grad_norm 3.9729 (4.9383)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:36:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][230/2433]	eta 0:30:33 lr 0.000000	 wd 0.0500	time 0.8317 (0.8324)	loss 0.5061 (0.6481)	grad_norm 4.0138 (4.8927)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][240/2433]	eta 0:30:25 lr 0.000000	 wd 0.0500	time 0.8326 (0.8325)	loss 0.4939 (0.6419)	grad_norm 4.0839 (4.8523)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][250/2433]	eta 0:30:17 lr 0.000000	 wd 0.0500	time 0.8322 (0.8325)	loss 0.4738 (0.6356)	grad_norm 3.6417 (4.8125)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][260/2433]	eta 0:30:09 lr 0.000000	 wd 0.0500	time 0.8361 (0.8325)	loss 0.4719 (0.6293)	grad_norm 3.6373 (4.7730)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][270/2433]	eta 0:30:00 lr 0.000000	 wd 0.0500	time 0.8361 (0.8326)	loss 0.4385 (0.6226)	grad_norm 3.6343 (4.7330)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][280/2433]	eta 0:29:52 lr 0.000000	 wd 0.0500	time 0.8397 (0.8327)	loss 0.4533 (0.6164)	grad_norm 3.5096 (4.6896)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][290/2433]	eta 0:29:44 lr 0.000000	 wd 0.0500	time 0.8349 (0.8327)	loss 0.4468 (0.6101)	grad_norm 3.2453 (4.6465)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:37:51 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][300/2433]	eta 0:29:36 lr 0.000000	 wd 0.0500	time 0.8372 (0.8328)	loss 0.3988 (0.6039)	grad_norm 3.2589 (4.6013)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][310/2433]	eta 0:29:28 lr 0.000000	 wd 0.0500	time 0.8365 (0.8329)	loss 0.3975 (0.5974)	grad_norm 3.2647 (4.5568)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][320/2433]	eta 0:29:20 lr 0.000000	 wd 0.0500	time 0.8366 (0.8330)	loss 0.4020 (0.5913)	grad_norm 3.2268 (4.5154)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][330/2433]	eta 0:29:11 lr 0.000000	 wd 0.0500	time 0.8346 (0.8330)	loss 0.3347 (0.5847)	grad_norm 3.1727 (4.4719)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:25 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][340/2433]	eta 0:29:03 lr 0.000000	 wd 0.0500	time 0.8315 (0.8331)	loss 0.3888 (0.5787)	grad_norm 2.9230 (4.4290)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:33 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][350/2433]	eta 0:28:55 lr 0.000000	 wd 0.0500	time 0.8338 (0.8332)	loss 0.3713 (0.5725)	grad_norm 2.9108 (4.3832)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][360/2433]	eta 0:28:47 lr 0.000000	 wd 0.0500	time 0.8364 (0.8333)	loss 0.3646 (0.5662)	grad_norm 2.8319 (4.3401)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:50 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][370/2433]	eta 0:28:39 lr 0.000000	 wd 0.0500	time 0.8350 (0.8334)	loss 0.3309 (0.5599)	grad_norm 2.6734 (4.2967)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:38:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][380/2433]	eta 0:28:31 lr 0.000000	 wd 0.0500	time 0.8329 (0.8335)	loss 0.2960 (0.5538)	grad_norm 2.8307 (4.2534)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:07 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][390/2433]	eta 0:28:23 lr 0.000000	 wd 0.0500	time 0.8377 (0.8336)	loss 0.3088 (0.5478)	grad_norm 2.3764 (4.2102)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:15 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][400/2433]	eta 0:28:14 lr 0.000000	 wd 0.0500	time 0.8363 (0.8337)	loss 0.3061 (0.5421)	grad_norm 2.4226 (4.1659)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][410/2433]	eta 0:28:06 lr 0.000000	 wd 0.0500	time 0.8348 (0.8338)	loss 0.3049 (0.5362)	grad_norm 2.3799 (4.1229)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][420/2433]	eta 0:27:58 lr 0.000000	 wd 0.0500	time 0.8340 (0.8339)	loss 0.2428 (0.5302)	grad_norm 2.2880 (4.0780)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][430/2433]	eta 0:27:50 lr 0.000000	 wd 0.0500	time 0.8407 (0.8340)	loss 0.2874 (0.5246)	grad_norm 2.0433 (4.0323)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][440/2433]	eta 0:27:42 lr 0.000000	 wd 0.0500	time 0.8375 (0.8341)	loss 0.2694 (0.5190)	grad_norm 2.0435 (3.9881)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:39:57 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][450/2433]	eta 0:27:34 lr 0.000000	 wd 0.0500	time 0.8390 (0.8342)	loss 0.2827 (0.5134)	grad_norm 1.8616 (3.9449)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][460/2433]	eta 0:27:26 lr 0.000000	 wd 0.0500	time 0.8414 (0.8343)	loss 0.2132 (0.5079)	grad_norm 1.8754 (3.9015)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][470/2433]	eta 0:27:17 lr 0.000000	 wd 0.0500	time 0.8407 (0.8344)	loss 0.2233 (0.5025)	grad_norm 1.9313 (3.8607)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][480/2433]	eta 0:27:09 lr 0.000000	 wd 0.0500	time 0.8375 (0.8345)	loss 0.2244 (0.4970)	grad_norm 1.8664 (3.8195)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][490/2433]	eta 0:27:01 lr 0.000000	 wd 0.0500	time 0.8423 (0.8347)	loss 0.2414 (0.4917)	grad_norm 1.8678 (3.7768)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:39 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][500/2433]	eta 0:26:53 lr 0.000000	 wd 0.0500	time 0.8411 (0.8348)	loss 0.2251 (0.4868)	grad_norm 1.9987 (3.7364)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][510/2433]	eta 0:26:45 lr 0.000000	 wd 0.0500	time 0.8396 (0.8349)	loss 0.2107 (0.4818)	grad_norm 1.6181 (3.6943)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:40:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][520/2433]	eta 0:26:37 lr 0.000000	 wd 0.0500	time 0.8375 (0.8349)	loss 0.2271 (0.4766)	grad_norm 1.4614 (3.6546)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][530/2433]	eta 0:26:29 lr 0.000000	 wd 0.0500	time 0.8417 (0.8350)	loss 0.2014 (0.4717)	grad_norm 1.3731 (3.6136)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][540/2433]	eta 0:26:20 lr 0.000000	 wd 0.0500	time 0.8391 (0.8351)	loss 0.2264 (0.4669)	grad_norm 1.4910 (3.5733)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:21 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][550/2433]	eta 0:26:12 lr 0.000000	 wd 0.0500	time 0.8401 (0.8352)	loss 0.2352 (0.4624)	grad_norm 1.1319 (3.5332)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][560/2433]	eta 0:26:04 lr 0.000000	 wd 0.0500	time 0.8417 (0.8353)	loss 0.1852 (0.4578)	grad_norm 1.3535 (3.4951)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][570/2433]	eta 0:25:56 lr 0.000000	 wd 0.0500	time 0.8386 (0.8353)	loss 0.2026 (0.4534)	grad_norm 1.2994 (3.4575)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][580/2433]	eta 0:25:47 lr 0.000000	 wd 0.0500	time 0.8371 (0.8354)	loss 0.1696 (0.4488)	grad_norm 1.3351 (3.4204)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:41:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][590/2433]	eta 0:25:39 lr 0.000000	 wd 0.0500	time 0.8434 (0.8355)	loss 0.2344 (0.4446)	grad_norm 1.1478 (3.3821)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:03 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][600/2433]	eta 0:25:31 lr 0.000000	 wd 0.0500	time 0.8390 (0.8355)	loss 0.1928 (0.4405)	grad_norm 1.1713 (3.3453)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][610/2433]	eta 0:25:23 lr 0.000000	 wd 0.0500	time 0.8389 (0.8356)	loss 0.2187 (0.4364)	grad_norm 0.9736 (3.3095)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][620/2433]	eta 0:25:15 lr 0.000000	 wd 0.0500	time 0.8379 (0.8356)	loss 0.2227 (0.4325)	grad_norm 1.1504 (3.2742)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][630/2433]	eta 0:25:06 lr 0.000000	 wd 0.0500	time 0.8396 (0.8357)	loss 0.1685 (0.4286)	grad_norm 1.1241 (3.2399)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][640/2433]	eta 0:24:58 lr 0.000000	 wd 0.0500	time 0.8380 (0.8357)	loss 0.2004 (0.4247)	grad_norm 1.1105 (3.2056)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:45 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][650/2433]	eta 0:24:50 lr 0.000000	 wd 0.0500	time 0.8398 (0.8358)	loss 0.1979 (0.4210)	grad_norm 1.0018 (3.1720)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:42:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][660/2433]	eta 0:24:41 lr 0.000000	 wd 0.0500	time 0.8413 (0.8358)	loss 0.1677 (0.4176)	grad_norm 0.9812 (3.1392)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:02 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][670/2433]	eta 0:24:33 lr 0.000000	 wd 0.0500	time 0.8402 (0.8359)	loss 0.1689 (0.4142)	grad_norm 1.0470 (3.1066)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][680/2433]	eta 0:24:25 lr 0.000000	 wd 0.0500	time 0.8382 (0.8359)	loss 0.1765 (0.4108)	grad_norm 1.0366 (3.0755)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][690/2433]	eta 0:24:17 lr 0.000000	 wd 0.0500	time 0.8414 (0.8360)	loss 0.1738 (0.4074)	grad_norm 0.9254 (3.0444)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:27 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][700/2433]	eta 0:24:08 lr 0.000000	 wd 0.0500	time 0.8415 (0.8360)	loss 0.1985 (0.4043)	grad_norm 0.9803 (3.0146)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][710/2433]	eta 0:24:00 lr 0.000000	 wd 0.0500	time 0.8349 (0.8361)	loss 0.2169 (0.4012)	grad_norm 0.9440 (2.9854)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:44 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][720/2433]	eta 0:23:52 lr 0.000000	 wd 0.0500	time 0.8417 (0.8361)	loss 0.1419 (0.3981)	grad_norm 0.9173 (2.9559)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:43:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][730/2433]	eta 0:23:44 lr 0.000000	 wd 0.0500	time 0.8392 (0.8362)	loss 0.1724 (0.3952)	grad_norm 0.9536 (2.9281)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][740/2433]	eta 0:23:35 lr 0.000001	 wd 0.0500	time 0.8361 (0.8363)	loss 0.1564 (0.3923)	grad_norm 0.8750 (2.9003)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:09 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][750/2433]	eta 0:23:27 lr 0.000001	 wd 0.0500	time 0.8431 (0.8363)	loss 0.1692 (0.3896)	grad_norm 0.8617 (2.8732)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:17 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][760/2433]	eta 0:23:19 lr 0.000001	 wd 0.0500	time 0.8406 (0.8363)	loss 0.1648 (0.3867)	grad_norm 0.8363 (2.8471)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:26 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][770/2433]	eta 0:23:10 lr 0.000001	 wd 0.0500	time 0.8390 (0.8364)	loss 0.1865 (0.3839)	grad_norm 0.7409 (2.8209)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:34 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][780/2433]	eta 0:23:02 lr 0.000001	 wd 0.0500	time 0.8402 (0.8364)	loss 0.1733 (0.3810)	grad_norm 0.8220 (2.7956)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][790/2433]	eta 0:22:54 lr 0.000001	 wd 0.0500	time 0.8371 (0.8365)	loss 0.2612 (0.3786)	grad_norm 1.1794 (2.7718)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:51 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][800/2433]	eta 0:22:45 lr 0.000001	 wd 0.0500	time 0.8373 (0.8365)	loss 0.1542 (0.3761)	grad_norm 0.9147 (2.7479)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:44:59 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][810/2433]	eta 0:22:37 lr 0.000001	 wd 0.0500	time 0.8377 (0.8365)	loss 0.1885 (0.3736)	grad_norm 0.7502 (2.7240)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:08 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][820/2433]	eta 0:22:29 lr 0.000001	 wd 0.0500	time 0.8368 (0.8366)	loss 0.2204 (0.3711)	grad_norm 0.9814 (2.7007)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][830/2433]	eta 0:22:21 lr 0.000001	 wd 0.0500	time 0.8381 (0.8366)	loss 0.2022 (0.3688)	grad_norm 0.9144 (2.6784)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][840/2433]	eta 0:22:12 lr 0.000001	 wd 0.0500	time 0.8403 (0.8366)	loss 0.1916 (0.3665)	grad_norm 0.9244 (2.6576)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:33 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][850/2433]	eta 0:22:04 lr 0.000001	 wd 0.0500	time 0.8380 (0.8367)	loss 0.1576 (0.3643)	grad_norm 0.7342 (2.6356)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][860/2433]	eta 0:21:56 lr 0.000001	 wd 0.0500	time 0.8411 (0.8367)	loss 0.1882 (0.3620)	grad_norm 0.9087 (2.6142)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:50 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][870/2433]	eta 0:21:47 lr 0.000001	 wd 0.0500	time 0.8431 (0.8367)	loss 0.1256 (0.3597)	grad_norm 0.6811 (2.5933)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:45:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][880/2433]	eta 0:21:39 lr 0.000001	 wd 0.0500	time 0.8400 (0.8368)	loss 0.1439 (0.3576)	grad_norm 0.8234 (2.5735)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][890/2433]	eta 0:21:31 lr 0.000001	 wd 0.0500	time 0.8446 (0.8368)	loss 0.1585 (0.3555)	grad_norm 0.7226 (2.5536)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:15 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][900/2433]	eta 0:21:22 lr 0.000001	 wd 0.0500	time 0.8408 (0.8369)	loss 0.1886 (0.3535)	grad_norm 0.9011 (2.5348)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][910/2433]	eta 0:21:14 lr 0.000001	 wd 0.0500	time 0.8391 (0.8369)	loss 0.2024 (0.3515)	grad_norm 0.8230 (2.5157)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][920/2433]	eta 0:21:06 lr 0.000001	 wd 0.0500	time 0.8446 (0.8369)	loss 0.1651 (0.3497)	grad_norm 0.9613 (2.4978)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][930/2433]	eta 0:20:57 lr 0.000001	 wd 0.0500	time 0.8403 (0.8370)	loss 0.1740 (0.3478)	grad_norm 0.7885 (2.4796)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][940/2433]	eta 0:20:49 lr 0.000001	 wd 0.0500	time 0.8448 (0.8370)	loss 0.2083 (0.3460)	grad_norm 1.0015 (2.4621)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:46:57 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][950/2433]	eta 0:20:41 lr 0.000001	 wd 0.0500	time 0.8409 (0.8370)	loss 0.1637 (0.3442)	grad_norm 0.8139 (2.4442)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][960/2433]	eta 0:20:32 lr 0.000001	 wd 0.0500	time 0.8381 (0.8370)	loss 0.1427 (0.3424)	grad_norm 0.6070 (2.4268)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][970/2433]	eta 0:20:24 lr 0.000001	 wd 0.0500	time 0.8370 (0.8371)	loss 0.1520 (0.3407)	grad_norm 0.6941 (2.4099)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][980/2433]	eta 0:20:16 lr 0.000001	 wd 0.0500	time 0.8410 (0.8371)	loss 0.1678 (0.3390)	grad_norm 1.0352 (2.3935)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][990/2433]	eta 0:20:07 lr 0.000001	 wd 0.0500	time 0.8387 (0.8371)	loss 0.1730 (0.3372)	grad_norm 0.7778 (2.3771)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:39 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1000/2433]	eta 0:19:59 lr 0.000001	 wd 0.0500	time 0.8399 (0.8372)	loss 0.2513 (0.3356)	grad_norm 1.0583 (2.3616)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1010/2433]	eta 0:19:51 lr 0.000001	 wd 0.0500	time 0.8419 (0.8372)	loss 0.1483 (0.3339)	grad_norm 0.6827 (2.3466)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:47:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1020/2433]	eta 0:19:42 lr 0.000001	 wd 0.0500	time 0.8359 (0.8372)	loss 0.1116 (0.3323)	grad_norm 0.6644 (2.3310)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1030/2433]	eta 0:19:34 lr 0.000001	 wd 0.0500	time 0.8416 (0.8372)	loss 0.2050 (0.3306)	grad_norm 0.8384 (2.3161)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:12 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1040/2433]	eta 0:19:26 lr 0.000001	 wd 0.0500	time 0.8395 (0.8372)	loss 0.1612 (0.3291)	grad_norm 0.6737 (2.3014)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:21 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1050/2433]	eta 0:19:17 lr 0.000001	 wd 0.0500	time 0.8383 (0.8373)	loss 0.1522 (0.3275)	grad_norm 0.5989 (2.2864)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1060/2433]	eta 0:19:09 lr 0.000001	 wd 0.0500	time 0.8403 (0.8373)	loss 0.1713 (0.3259)	grad_norm 0.6907 (2.2720)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:37 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1070/2433]	eta 0:19:01 lr 0.000001	 wd 0.0500	time 0.8374 (0.8373)	loss 0.1898 (0.3244)	grad_norm 1.0738 (2.2582)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:46 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1080/2433]	eta 0:18:52 lr 0.000001	 wd 0.0500	time 0.8387 (0.8373)	loss 0.2123 (0.3232)	grad_norm 0.7834 (2.2445)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:48:54 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1090/2433]	eta 0:18:44 lr 0.000001	 wd 0.0500	time 0.8381 (0.8374)	loss 0.1374 (0.3218)	grad_norm 0.6684 (2.2312)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:03 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1100/2433]	eta 0:18:36 lr 0.000001	 wd 0.0500	time 0.8393 (0.8374)	loss 0.1353 (0.3203)	grad_norm 0.8491 (2.2175)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:11 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1110/2433]	eta 0:18:27 lr 0.000001	 wd 0.0500	time 0.8397 (0.8374)	loss 0.1584 (0.3191)	grad_norm 0.6568 (2.2050)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:19 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1120/2433]	eta 0:18:19 lr 0.000001	 wd 0.0500	time 0.8424 (0.8374)	loss 0.2001 (0.3178)	grad_norm 0.7083 (2.1919)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:28 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1130/2433]	eta 0:18:11 lr 0.000001	 wd 0.0500	time 0.8408 (0.8374)	loss 0.1677 (0.3166)	grad_norm 0.6030 (2.1788)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:36 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1140/2433]	eta 0:18:02 lr 0.000001	 wd 0.0500	time 0.8380 (0.8375)	loss 0.1545 (0.3151)	grad_norm 0.6123 (2.1670)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:45 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1150/2433]	eta 0:17:54 lr 0.000001	 wd 0.0500	time 0.8386 (0.8375)	loss 0.2021 (0.3140)	grad_norm 0.7083 (2.1555)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:49:53 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1160/2433]	eta 0:17:46 lr 0.000001	 wd 0.0500	time 0.8354 (0.8375)	loss 0.1251 (0.3128)	grad_norm 0.6321 (2.1430)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:01 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1170/2433]	eta 0:17:37 lr 0.000001	 wd 0.0500	time 0.8413 (0.8375)	loss 0.1556 (0.3115)	grad_norm 0.7008 (2.1309)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:10 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1180/2433]	eta 0:17:29 lr 0.000001	 wd 0.0500	time 0.8383 (0.8375)	loss 0.2028 (0.3103)	grad_norm 0.9567 (2.1196)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:18 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1190/2433]	eta 0:17:21 lr 0.000001	 wd 0.0500	time 0.8404 (0.8375)	loss 0.1706 (0.3091)	grad_norm 0.5869 (2.1071)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:27 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1200/2433]	eta 0:17:12 lr 0.000001	 wd 0.0500	time 0.8354 (0.8375)	loss 0.1708 (0.3079)	grad_norm 0.7333 (2.0954)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:35 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1210/2433]	eta 0:17:04 lr 0.000001	 wd 0.0500	time 0.8378 (0.8375)	loss 0.1451 (0.3066)	grad_norm 0.6630 (2.0842)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:43 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1220/2433]	eta 0:16:55 lr 0.000001	 wd 0.0500	time 0.8400 (0.8376)	loss 0.1683 (0.3056)	grad_norm 0.8325 (2.0738)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:50:52 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1230/2433]	eta 0:16:47 lr 0.000001	 wd 0.0500	time 0.8386 (0.8376)	loss 0.2028 (0.3044)	grad_norm 0.9398 (2.0629)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:00 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1240/2433]	eta 0:16:39 lr 0.000001	 wd 0.0500	time 0.8341 (0.8376)	loss 0.2935 (0.3033)	grad_norm 1.1691 (2.0527)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:09 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1250/2433]	eta 0:16:30 lr 0.000001	 wd 0.0500	time 0.8441 (0.8376)	loss 0.2166 (0.3023)	grad_norm 0.7380 (2.0424)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:17 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1260/2433]	eta 0:16:22 lr 0.000001	 wd 0.0500	time 0.8441 (0.8376)	loss 0.1317 (0.3012)	grad_norm 0.7332 (2.0320)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:25 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1270/2433]	eta 0:16:14 lr 0.000001	 wd 0.0500	time 0.8422 (0.8376)	loss 0.1983 (0.3001)	grad_norm 0.9666 (2.0221)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:34 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1280/2433]	eta 0:16:05 lr 0.000001	 wd 0.0500	time 0.8395 (0.8376)	loss 0.1705 (0.2991)	grad_norm 0.6403 (2.0123)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:42 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1290/2433]	eta 0:15:57 lr 0.000001	 wd 0.0500	time 0.8416 (0.8377)	loss 0.1111 (0.2980)	grad_norm 0.7369 (2.0025)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:51 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1300/2433]	eta 0:15:49 lr 0.000001	 wd 0.0500	time 0.8369 (0.8377)	loss 0.1358 (0.2970)	grad_norm 0.6074 (1.9926)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:51:59 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1310/2433]	eta 0:15:40 lr 0.000001	 wd 0.0500	time 0.8389 (0.8377)	loss 0.1702 (0.2960)	grad_norm 0.6346 (1.9824)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:07 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1320/2433]	eta 0:15:32 lr 0.000001	 wd 0.0500	time 0.8390 (0.8377)	loss 0.1446 (0.2950)	grad_norm 0.6725 (1.9726)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:16 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1330/2433]	eta 0:15:23 lr 0.000001	 wd 0.0500	time 0.8375 (0.8377)	loss 0.1784 (0.2941)	grad_norm 0.7303 (1.9635)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:24 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1340/2433]	eta 0:15:15 lr 0.000001	 wd 0.0500	time 0.8380 (0.8377)	loss 0.1207 (0.2930)	grad_norm 0.5974 (1.9539)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:32 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1350/2433]	eta 0:15:07 lr 0.000001	 wd 0.0500	time 0.8362 (0.8377)	loss 0.1062 (0.2921)	grad_norm 0.5245 (1.9448)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:41 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1360/2433]	eta 0:14:58 lr 0.000001	 wd 0.0500	time 0.8399 (0.8377)	loss 0.1864 (0.2911)	grad_norm 0.7357 (1.9361)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:49 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1370/2433]	eta 0:14:50 lr 0.000001	 wd 0.0500	time 0.8366 (0.8377)	loss 0.1804 (0.2903)	grad_norm 0.5595 (1.9276)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:52:58 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1380/2433]	eta 0:14:42 lr 0.000001	 wd 0.0500	time 0.8388 (0.8377)	loss 0.1228 (0.2893)	grad_norm 0.8812 (1.9193)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:06 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1390/2433]	eta 0:14:33 lr 0.000001	 wd 0.0500	time 0.8373 (0.8377)	loss 0.1611 (0.2885)	grad_norm 0.6989 (1.9109)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:14 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1400/2433]	eta 0:14:25 lr 0.000001	 wd 0.0500	time 0.8403 (0.8378)	loss 0.1906 (0.2877)	grad_norm 0.7793 (1.9026)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:23 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1410/2433]	eta 0:14:17 lr 0.000001	 wd 0.0500	time 0.8373 (0.8378)	loss 0.1562 (0.2869)	grad_norm 0.7116 (1.8946)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:31 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1420/2433]	eta 0:14:08 lr 0.000001	 wd 0.0500	time 0.8385 (0.8378)	loss 0.1444 (0.2859)	grad_norm 0.6377 (1.8861)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:40 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1430/2433]	eta 0:14:00 lr 0.000001	 wd 0.0500	time 0.8380 (0.8378)	loss 0.2078 (0.2851)	grad_norm 0.9620 (1.8786)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:48 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1440/2433]	eta 0:13:51 lr 0.000001	 wd 0.0500	time 0.8374 (0.8378)	loss 0.1930 (0.2843)	grad_norm 0.8107 (1.8710)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:53:56 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1450/2433]	eta 0:13:43 lr 0.000001	 wd 0.0500	time 0.8410 (0.8378)	loss 0.1707 (0.2835)	grad_norm 0.7002 (1.8631)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:05 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1460/2433]	eta 0:13:35 lr 0.000001	 wd 0.0500	time 0.8396 (0.8378)	loss 0.1106 (0.2827)	grad_norm 1.0891 (1.8560)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:13 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1470/2433]	eta 0:13:26 lr 0.000001	 wd 0.0500	time 0.8361 (0.8379)	loss 0.1449 (0.2819)	grad_norm 0.6518 (1.8486)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:22 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1480/2433]	eta 0:13:18 lr 0.000001	 wd 0.0500	time 0.8430 (0.8379)	loss 0.1991 (0.2811)	grad_norm 0.8183 (1.8411)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:30 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1490/2433]	eta 0:13:10 lr 0.000001	 wd 0.0500	time 0.8404 (0.8379)	loss 0.1970 (0.2804)	grad_norm 0.8046 (1.8333)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:38 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1500/2433]	eta 0:13:01 lr 0.000001	 wd 0.0500	time 0.8366 (0.8379)	loss 0.1706 (0.2796)	grad_norm 0.7026 (1.8261)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:47 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1510/2433]	eta 0:12:53 lr 0.000001	 wd 0.0500	time 0.8422 (0.8379)	loss 0.2135 (0.2788)	grad_norm 0.6656 (1.8186)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:54:55 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1520/2433]	eta 0:12:45 lr 0.000001	 wd 0.0500	time 0.8419 (0.8379)	loss 0.2710 (0.2781)	grad_norm 1.2190 (1.8116)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:55:04 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1530/2433]	eta 0:12:36 lr 0.000001	 wd 0.0500	time 0.8364 (0.8379)	loss 0.1517 (0.2773)	grad_norm 0.6283 (1.8049)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:55:12 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1540/2433]	eta 0:12:28 lr 0.000001	 wd 0.0500	time 0.8426 (0.8380)	loss 0.1290 (0.2766)	grad_norm 0.5790 (1.7978)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:55:20 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1550/2433]	eta 0:12:19 lr 0.000001	 wd 0.0500	time 0.8439 (0.8380)	loss 0.1487 (0.2760)	grad_norm 0.6550 (1.7915)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:55:29 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1560/2433]	eta 0:12:11 lr 0.000001	 wd 0.0500	time 0.8368 (0.8380)	loss 0.2187 (0.2753)	grad_norm 0.8121 (1.7845)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
[2024-03-21 18:55:37 swin_base_patch4_window7_224] (main_classification_ddp.py 410): INFO Train: [0/150][1570/2433]	eta 0:12:03 lr 0.000001	 wd 0.0500	time 0.8404 (0.8380)	loss 0.1322 (0.2745)	grad_norm 0.7094 (1.7777)	loss_scale 65536.0000 (65536.0000)	mem 6085MB
